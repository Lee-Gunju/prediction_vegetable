{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd06d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (2.4.1)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.2)\nRequirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.14.0)\nRequirement already satisfied: gast==0.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.3.3)\nRequirement already satisfied: tensorboard~=2.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.4.0)\nRequirement already satisfied: grpcio~=1.32.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.32.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12)\nRequirement already satisfied: wrapt~=1.12.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12.1)\nRequirement already satisfied: google-pasta~=0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.2.0)\nRequirement already satisfied: absl-py~=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.11.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\nRequirement already satisfied: termcolor~=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.0)Note: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: numpy~=1.19.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.19.2)\nRequirement already satisfied: six~=1.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\nRequirement already satisfied: h5py~=2.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.10.0)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.4.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.3.0)\nRequirement already satisfied: astunparse~=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.6.3)\nRequirement already satisfied: wheel~=0.35 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.35.1)\nRequirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.3)\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.7.0)\nRequirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (50.3.1.post20201107)\nRequirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.24.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.2)\nRequirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.24.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.0)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.6)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.25.11)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.0)\n\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from pandas.io.parsers import read_csv\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv('price data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array(data, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2922, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "x_data = xy[:, 1:-1]\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizers.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.SGD(lr = 0.0000005)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape = (4,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse', optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_16 (Dense)             (None, 4)                 20        \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 5         \n=================================================================\nTotal params: 25\nTrainable params: 25\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37.6882\n",
      "Epoch 781/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1789803.8508\n",
      "Epoch 782/1000\n",
      "92/92 [==============================] - 0s 890us/step - loss: 1944541.7473\n",
      "Epoch 783/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1928561.3011\n",
      "Epoch 784/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1801435.2876\n",
      "Epoch 785/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1953552.6855\n",
      "Epoch 786/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1883061.9368\n",
      "Epoch 787/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1874434.1667\n",
      "Epoch 788/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 2072132.9718\n",
      "Epoch 789/1000\n",
      "92/92 [==============================] - 0s 648us/step - loss: 1924323.3858\n",
      "Epoch 790/1000\n",
      "92/92 [==============================] - 0s 604us/step - loss: 1837087.8522\n",
      "Epoch 791/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1887838.0699\n",
      "Epoch 792/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1976115.4032\n",
      "Epoch 793/1000\n",
      "92/92 [==============================] - 0s 670us/step - loss: 1898747.5860\n",
      "Epoch 794/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1900761.6949\n",
      "Epoch 795/1000\n",
      "92/92 [==============================] - 0s 626us/step - loss: 1887208.3750\n",
      "Epoch 796/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1764842.7715\n",
      "Epoch 797/1000\n",
      "92/92 [==============================] - 0s 648us/step - loss: 2040472.5524\n",
      "Epoch 798/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1963155.7648\n",
      "Epoch 799/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1750981.2446\n",
      "Epoch 800/1000\n",
      "92/92 [==============================] - 0s 835us/step - loss: 1899083.3333\n",
      "Epoch 801/1000\n",
      "92/92 [==============================] - 0s 890us/step - loss: 2006494.5753\n",
      "Epoch 802/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1923375.5242\n",
      "Epoch 803/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1778121.9341\n",
      "Epoch 804/1000\n",
      "92/92 [==============================] - 0s 670us/step - loss: 1763976.6089\n",
      "Epoch 805/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1920850.6815\n",
      "Epoch 806/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1992853.8750\n",
      "Epoch 807/1000\n",
      "92/92 [==============================] - 0s 846us/step - loss: 1731414.2177\n",
      "Epoch 808/1000\n",
      "92/92 [==============================] - 0s 637us/step - loss: 2076203.1841\n",
      "Epoch 809/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1936382.2083\n",
      "Epoch 810/1000\n",
      "92/92 [==============================] - 0s 637us/step - loss: 1769221.1169\n",
      "Epoch 811/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1962753.2930\n",
      "Epoch 812/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1783044.5336\n",
      "Epoch 813/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1851811.5202\n",
      "Epoch 814/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1944921.1989\n",
      "Epoch 815/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1858405.2876\n",
      "Epoch 816/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1691948.7890\n",
      "Epoch 817/1000\n",
      "92/92 [==============================] - 0s 868us/step - loss: 2074820.4220\n",
      "Epoch 818/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1976104.4449\n",
      "Epoch 819/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1894660.7339\n",
      "Epoch 820/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1887358.0296\n",
      "Epoch 821/1000\n",
      "92/92 [==============================] - 0s 615us/step - loss: 1720421.2567\n",
      "Epoch 822/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1884454.1035\n",
      "Epoch 823/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1705610.5188\n",
      "Epoch 824/1000\n",
      "92/92 [==============================] - 0s 582us/step - loss: 1831758.2675\n",
      "Epoch 825/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1793813.2648\n",
      "Epoch 826/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1878794.3226\n",
      "Epoch 827/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1969429.4395\n",
      "Epoch 828/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1822026.8387\n",
      "Epoch 829/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1803549.6855\n",
      "Epoch 830/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1868041.1089\n",
      "Epoch 831/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 2025690.0269\n",
      "Epoch 832/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1748143.9651\n",
      "Epoch 833/1000\n",
      "92/92 [==============================] - 0s 615us/step - loss: 2003259.1317\n",
      "Epoch 834/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1708333.6734\n",
      "Epoch 835/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1807231.0948\n",
      "Epoch 836/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1858595.8360\n",
      "Epoch 837/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1819908.1841\n",
      "Epoch 838/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1866114.1304\n",
      "Epoch 839/1000\n",
      "92/92 [==============================] - 0s 956us/step - loss: 1896761.3884\n",
      "Epoch 840/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1807055.2137\n",
      "Epoch 841/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1755800.8118\n",
      "Epoch 842/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1791755.5121\n",
      "Epoch 843/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1909711.7715\n",
      "Epoch 844/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1839207.3589\n",
      "Epoch 845/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1977873.9691\n",
      "Epoch 846/1000\n",
      "92/92 [==============================] - 0s 879us/step - loss: 1931386.8118\n",
      "Epoch 847/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1814027.4274\n",
      "Epoch 848/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1810537.0914\n",
      "Epoch 849/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1823499.5806\n",
      "Epoch 850/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1816344.0255\n",
      "Epoch 851/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1850483.6169\n",
      "Epoch 852/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1817455.1788\n",
      "Epoch 853/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1868779.2984\n",
      "Epoch 854/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1792132.6599\n",
      "Epoch 855/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1909280.1156\n",
      "Epoch 856/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 2169504.6505\n",
      "Epoch 857/1000\n",
      "92/92 [==============================] - 0s 912us/step - loss: 1788321.3280\n",
      "Epoch 858/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1988197.7137\n",
      "Epoch 859/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1911567.0712\n",
      "Epoch 860/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1787925.9583\n",
      "Epoch 861/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1730477.6653\n",
      "Epoch 862/1000\n",
      "92/92 [==============================] - 0s 1000us/step - loss: 1791368.9220\n",
      "Epoch 863/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1752690.5255\n",
      "Epoch 864/1000\n",
      "92/92 [==============================] - 0s 934us/step - loss: 1905736.3817\n",
      "Epoch 865/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1889455.4691\n",
      "Epoch 866/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1801638.2594\n",
      "Epoch 867/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1841019.5296\n",
      "Epoch 868/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1978173.9758\n",
      "Epoch 869/1000\n",
      "92/92 [==============================] - 0s 637us/step - loss: 1869025.8763\n",
      "Epoch 870/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1828740.1633\n",
      "Epoch 871/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1856069.7124\n",
      "Epoch 872/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1864224.2796\n",
      "Epoch 873/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1850206.0645\n",
      "Epoch 874/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1975578.2487\n",
      "Epoch 875/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 2002437.0121\n",
      "Epoch 876/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1818979.7997\n",
      "Epoch 877/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1953248.6035\n",
      "Epoch 878/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1884857.2164\n",
      "Epoch 879/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1872225.4758\n",
      "Epoch 880/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1857402.4691\n",
      "Epoch 881/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1824242.3723\n",
      "Epoch 882/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1869174.7715\n",
      "Epoch 883/1000\n",
      "92/92 [==============================] - 0s 956us/step - loss: 1964844.1667\n",
      "Epoch 884/1000\n",
      "92/92 [==============================] - 0s 846us/step - loss: 1976332.9341\n",
      "Epoch 885/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1904027.5766\n",
      "Epoch 886/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1876250.2003\n",
      "Epoch 887/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1810925.0323\n",
      "Epoch 888/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1990418.4745\n",
      "Epoch 889/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1823169.6331\n",
      "Epoch 890/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1890699.8212\n",
      "Epoch 891/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1822641.1371\n",
      "Epoch 892/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1981688.2554\n",
      "Epoch 893/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1874678.8065\n",
      "Epoch 894/1000\n",
      "92/92 [==============================] - 0s 835us/step - loss: 1855633.1532\n",
      "Epoch 895/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1919380.9032\n",
      "Epoch 896/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1785429.3051\n",
      "Epoch 897/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1781484.4368\n",
      "Epoch 898/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1866785.3602\n",
      "Epoch 899/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 2058946.7608\n",
      "Epoch 900/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1813983.7419\n",
      "Epoch 901/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1844127.9973\n",
      "Epoch 902/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1748778.8266\n",
      "Epoch 903/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1934872.9281\n",
      "Epoch 904/1000\n",
      "92/92 [==============================] - 0s 912us/step - loss: 1887961.1358\n",
      "Epoch 905/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1932830.3589\n",
      "Epoch 906/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1701233.0726\n",
      "Epoch 907/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 2053071.2809\n",
      "Epoch 908/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1736002.0242\n",
      "Epoch 909/1000\n",
      "92/92 [==============================] - 0s 923us/step - loss: 1793176.9422\n",
      "Epoch 910/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1872407.6680\n",
      "Epoch 911/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1866253.1976\n",
      "Epoch 912/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1790568.6142\n",
      "Epoch 913/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1819922.5094\n",
      "Epoch 914/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1769487.0134\n",
      "Epoch 915/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1850165.4704\n",
      "Epoch 916/1000\n",
      "92/92 [==============================] - 0s 846us/step - loss: 1854187.1801\n",
      "Epoch 917/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1866569.8387\n",
      "Epoch 918/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1775059.1371\n",
      "Epoch 919/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1796512.6142\n",
      "Epoch 920/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1854292.2319\n",
      "Epoch 921/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1674353.0027\n",
      "Epoch 922/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1914722.1129\n",
      "Epoch 923/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1796927.0995\n",
      "Epoch 924/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1853873.2863\n",
      "Epoch 925/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 2037314.4798\n",
      "Epoch 926/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 2166893.6653\n",
      "Epoch 927/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1826892.7823\n",
      "Epoch 928/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1905838.7218\n",
      "Epoch 929/1000\n",
      "92/92 [==============================] - 0s 670us/step - loss: 1918008.2379\n",
      "Epoch 930/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1928115.9301\n",
      "Epoch 931/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1701644.1183\n",
      "Epoch 932/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1834476.1317\n",
      "Epoch 933/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1787037.2406\n",
      "Epoch 934/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1765482.1183\n",
      "Epoch 935/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1913337.0981\n",
      "Epoch 936/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1785755.8575\n",
      "Epoch 937/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1869715.8750\n",
      "Epoch 938/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1751783.3723\n",
      "Epoch 939/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1741336.6855\n",
      "Epoch 940/1000\n",
      "92/92 [==============================] - 0s 846us/step - loss: 1749553.9503\n",
      "Epoch 941/1000\n",
      "92/92 [==============================] - 0s 648us/step - loss: 1920465.6734\n",
      "Epoch 942/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1748934.0712\n",
      "Epoch 943/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1910999.0645\n",
      "Epoch 944/1000\n",
      "92/92 [==============================] - 0s 978us/step - loss: 1752803.3253\n",
      "Epoch 945/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1894204.8723\n",
      "Epoch 946/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1890021.2930\n",
      "Epoch 947/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1915696.6976\n",
      "Epoch 948/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1838074.2265\n",
      "Epoch 949/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1842119.3414\n",
      "Epoch 950/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1793396.6048\n",
      "Epoch 951/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1900725.6694\n",
      "Epoch 952/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1872618.2903\n",
      "Epoch 953/1000\n",
      "92/92 [==============================] - 0s 626us/step - loss: 1800961.3091\n",
      "Epoch 954/1000\n",
      "92/92 [==============================] - 0s 670us/step - loss: 1713164.3172\n",
      "Epoch 955/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1840852.0632\n",
      "Epoch 956/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1844610.4543\n",
      "Epoch 957/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1870159.8642\n",
      "Epoch 958/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1933976.6465\n",
      "Epoch 959/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1924482.8212\n",
      "Epoch 960/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1910517.8132\n",
      "Epoch 961/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1883464.2487\n",
      "Epoch 962/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1933012.5188\n",
      "Epoch 963/1000\n",
      "92/92 [==============================] - 0s 736us/step - loss: 1970672.2110\n",
      "Epoch 964/1000\n",
      "92/92 [==============================] - 0s 659us/step - loss: 1812234.5692\n",
      "Epoch 965/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1883185.1949\n",
      "Epoch 966/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 2008075.4987\n",
      "Epoch 967/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1840176.8320\n",
      "Epoch 968/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1902386.2997\n",
      "Epoch 969/1000\n",
      "92/92 [==============================] - 0s 703us/step - loss: 1719379.5309\n",
      "Epoch 970/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1870036.8696\n",
      "Epoch 971/1000\n",
      "92/92 [==============================] - 0s 835us/step - loss: 1832372.2594\n",
      "Epoch 972/1000\n",
      "92/92 [==============================] - 0s 879us/step - loss: 1950254.5685\n",
      "Epoch 973/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1910270.4691\n",
      "Epoch 974/1000\n",
      "92/92 [==============================] - 0s 758us/step - loss: 1752800.0578\n",
      "Epoch 975/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1905979.1909\n",
      "Epoch 976/1000\n",
      "92/92 [==============================] - 0s 923us/step - loss: 1860577.0202\n",
      "Epoch 977/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1863146.1384\n",
      "Epoch 978/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1923889.1505\n",
      "Epoch 979/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 2003584.8575\n",
      "Epoch 980/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1845104.5901\n",
      "Epoch 981/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1741876.6788\n",
      "Epoch 982/1000\n",
      "92/92 [==============================] - 0s 692us/step - loss: 1776934.4449\n",
      "Epoch 983/1000\n",
      "92/92 [==============================] - 0s 857us/step - loss: 1937365.4758\n",
      "Epoch 984/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1771041.3165\n",
      "Epoch 985/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1843355.7204\n",
      "Epoch 986/1000\n",
      "92/92 [==============================] - 0s 813us/step - loss: 1908161.4274\n",
      "Epoch 987/1000\n",
      "92/92 [==============================] - 0s 714us/step - loss: 1918744.7030\n",
      "Epoch 988/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1722617.9167\n",
      "Epoch 989/1000\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 1816863.1492\n",
      "Epoch 990/1000\n",
      "92/92 [==============================] - 0s 780us/step - loss: 1883330.4073\n",
      "Epoch 991/1000\n",
      "92/92 [==============================] - 0s 868us/step - loss: 1737102.5134\n",
      "Epoch 992/1000\n",
      "92/92 [==============================] - 0s 989us/step - loss: 1822164.7191\n",
      "Epoch 993/1000\n",
      "92/92 [==============================] - 0s 824us/step - loss: 1952128.2997\n",
      "Epoch 994/1000\n",
      "92/92 [==============================] - 0s 802us/step - loss: 1921465.6210\n",
      "Epoch 995/1000\n",
      "92/92 [==============================] - 0s 835us/step - loss: 1760207.9113\n",
      "Epoch 996/1000\n",
      "92/92 [==============================] - 0s 725us/step - loss: 1938353.3522\n",
      "Epoch 997/1000\n",
      "92/92 [==============================] - 0s 769us/step - loss: 1882615.1613\n",
      "Epoch 998/1000\n",
      "92/92 [==============================] - 0s 747us/step - loss: 1858849.2433\n",
      "Epoch 999/1000\n",
      "92/92 [==============================] - 0s 791us/step - loss: 1877591.7540\n",
      "Epoch 1000/1000\n",
      "92/92 [==============================] - 0s 681us/step - loss: 1727339.3226\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_data, y_data, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2856.7903]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "x = np.array(x)\n",
    "x = tf.expand_dims(x, 0)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved.cpkt\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}